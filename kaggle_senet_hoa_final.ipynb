{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeIzvmkFeVBz"
      },
      "source": [
        "## setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1P5hod_LM4Ff"
      },
      "outputs": [],
      "source": [
        "#!pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ofku2V8WM5Cu"
      },
      "outputs": [],
      "source": [
        "#!mkdir -p ~/.kaggle\n",
        "#!cp /content/drive/MyDrive/Kaggle/kaggle.json ~/.kaggle/\n",
        "#!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_k9JDfObMXAe"
      },
      "outputs": [],
      "source": [
        "#!kaggle competitions download -c blood-vessel-segmentation -p \"/content/drive/MyDrive/Kaggle/SenNet + HOA - Hacking the Human Vasculature in 3D/DataSources\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OuISFCBUM4G9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "if not os.path.isdir('/content/train'):\n",
        "    !unzip \"/content/drive/MyDrive/Kaggle/SenNet + HOA - Hacking the Human Vasculature in 3D/DataSources/blood-vessel-segmentation.zip\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzRhUsedfQFM"
      },
      "source": [
        "## library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mljc26bNfRG2",
        "outputId": "ad72ff9e-2c6a-405b-e355-2befe524df19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (0.9.2)\n",
            "Requirement already satisfied: torch>=1.7 in /usr/local/lib/python3.10/dist-packages (from timm) (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.16.0+cu121)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.20.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (4.66.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (23.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.23.5)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7->timm) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7->timm) (1.3.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n",
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.10/dist-packages (1.3.1)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.11.4)\n",
            "Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.19.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from albumentations) (6.0.1)\n",
            "Requirement already satisfied: qudida>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.0.4)\n",
            "Requirement already satisfied: opencv-python-headless>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from albumentations) (4.8.1.78)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from qudida>=0.0.4->albumentations) (1.2.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from qudida>=0.0.4->albumentations) (4.5.0)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations) (3.2.1)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations) (9.4.0)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations) (2.31.6)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations) (2023.12.9)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations) (1.5.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations) (23.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (3.2.0)\n",
            "Requirement already satisfied: segmentation_models_pytorch in /usr/local/lib/python3.10/dist-packages (0.3.3)\n",
            "Requirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (0.16.0+cu121)\n",
            "Requirement already satisfied: pretrainedmodels==0.7.4 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (0.7.4)\n",
            "Requirement already satisfied: efficientnet-pytorch==0.7.1 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (0.7.1)\n",
            "Requirement already satisfied: timm==0.9.2 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (0.9.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (4.66.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (9.4.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2.1.0+cu121)\n",
            "Requirement already satisfied: munch in /usr/local/lib/python3.10/dist-packages (from pretrainedmodels==0.7.4->segmentation_models_pytorch) (4.0.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm==0.9.2->segmentation_models_pytorch) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from timm==0.9.2->segmentation_models_pytorch) (0.20.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm==0.9.2->segmentation_models_pytorch) (0.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.5.0->segmentation_models_pytorch) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.5.0->segmentation_models_pytorch) (2.31.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2.1.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm==0.9.2->segmentation_models_pytorch) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install timm\n",
        "!pip install transformers\n",
        "!pip install -U albumentations\n",
        "!pip install segmentation_models_pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FeuTaTuBfcwQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import gc\n",
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "import tifffile\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchvision\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "\n",
        "import timm\n",
        "\n",
        "from transformers.optimization import get_cosine_schedule_with_warmup\n",
        "\n",
        "import albumentations as A\n",
        "\n",
        "import segmentation_models_pytorch as smp\n",
        "from segmentation_models_pytorch.decoders.unet.model import (\n",
        "    UnetDecoder,\n",
        "    SegmentationHead,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EDoZV0znehH"
      },
      "source": [
        "## utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPiyPwo0nfa-"
      },
      "outputs": [],
      "source": [
        "# ref.: https://www.kaggle.com/stainsby/fast-tested-rle\n",
        "def rle_encode(img):\n",
        "    '''\n",
        "    img: numpy array, 1 - mask, 0 - background\n",
        "    Returns run length as string formated\n",
        "    '''\n",
        "    pixels = img.flatten()\n",
        "    pixels = np.concatenate([[0], pixels, [0]])\n",
        "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
        "    runs[1::2] -= runs[::2]\n",
        "    return ' '.join(str(x) for x in runs)\n",
        "\n",
        "def rle_decode(mask_rle, shape):\n",
        "    '''\n",
        "    mask_rle: run-length as string formated (start length)\n",
        "    shape: (height,width) of array to return\n",
        "    Returns numpy array, 1 - mask, 0 - background\n",
        "\n",
        "    '''\n",
        "    s = mask_rle.split()\n",
        "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
        "    starts -= 1\n",
        "    ends = starts + lengths\n",
        "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
        "    for lo, hi in zip(starts, ends):\n",
        "        img[lo:hi] = 1\n",
        "    return img.reshape(shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JfD_9vVF0W9p"
      },
      "outputs": [],
      "source": [
        "%run '/content/drive/MyDrive/Kaggle/SenNet + HOA - Hacking the Human Vasculature in 3D/metric.py'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-J4mkjLEUYOV"
      },
      "outputs": [],
      "source": [
        "# ref.: https://www.kaggle.com/code/iafoss/unet34-dice-0-87/notebook\n",
        "\n",
        "def dice_score(pred, targs):\n",
        "    pred = (pred>0).float()\n",
        "    return 2.0 * (pred*targs).sum() / ((pred+targs).sum() + 1.0)\n",
        "\n",
        "def IoU_score(pred, targs):\n",
        "    pred = (pred>0).float()\n",
        "    intersection = (pred*targs).sum()\n",
        "    return intersection / ((pred+targs).sum() - intersection + 1.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzBuBvySWwcw"
      },
      "source": [
        "## score function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PRr-iFC4Wiv8"
      },
      "outputs": [],
      "source": [
        "# PyTorch version dependence on index data type\n",
        "torch_ver_major = int(torch.__version__.split('.')[0])\n",
        "dtype_index = torch.int32 if torch_ver_major >= 2 else torch.long\n",
        "\n",
        "def compute_area(y: list, unfold: nn.Unfold, area: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      y (list[Tensor]): A pair of consecutive slices of mask\n",
        "      unfold: nn.Unfold(kernel_size=(2, 2), padding=1)\n",
        "      area (Tensor): surface area for 256 patterns (256, )\n",
        "\n",
        "    Returns:\n",
        "      Surface area of surface in 2x2x2 cube\n",
        "    \"\"\"\n",
        "    # Two layers of segmentation masks\n",
        "    yy = torch.stack(y, dim=0).to(torch.float16).unsqueeze(0)\n",
        "    # (batch_size=1, nch=2, H, W)\n",
        "    # bit (0/1) but unfold requires float\n",
        "\n",
        "    # unfold slides through the volume like a convolution\n",
        "    # 2x2 kernel returns 8 values (2 channels * 2x2)\n",
        "    cubes_float = unfold(yy).squeeze(0)  # (8, n_cubes)\n",
        "\n",
        "    # Each of the 8 values are either 0 or 1\n",
        "    # Convert those 8 bits to one uint8\n",
        "    cubes_byte = torch.zeros(cubes_float.size(1), dtype=dtype_index, device=device)\n",
        "    # indices are required to be int32 or long for area[cube_byte] below, not uint8\n",
        "    # Can be int32 for torch 2.0.0, int32 raise IndexError in torch 1.13.1.\n",
        "\n",
        "    for k in range(8):\n",
        "        cubes_byte += cubes_float[k, :].to(dtype_index) << k\n",
        "\n",
        "    # Use area lookup table: pattern index -> area [float]\n",
        "    cubes_area = area[cubes_byte]\n",
        "\n",
        "    return cubes_area\n",
        "\n",
        "\n",
        "def compute_surface_dice_score(submit: pd.DataFrame, label: pd.DataFrame) -> float:\n",
        "    \"\"\"\n",
        "    Compute surface Dice score for one 3D volume\n",
        "\n",
        "    submit (pd.DataFrame): submission file with id and rle\n",
        "    label (pd.DataFrame): ground truth id, rle, and also image height, width\n",
        "    \"\"\"\n",
        "    # submit and label must contain exact same id in same order\n",
        "    assert (submit['id'] == label['id']).all()\n",
        "    assert len(label) > 0\n",
        "\n",
        "    # All height, width must be the same\n",
        "    len(label['height'].unique()) == 1\n",
        "    len(label['width'].unique()) == 1\n",
        "\n",
        "    # Surface area lookup table: Tensor[float32] (256, )\n",
        "    area = create_table_neighbour_code_to_surface_area((1, 1, 1))\n",
        "    area = torch.from_numpy(area).to(device)  # torch.float32\n",
        "\n",
        "    # Slide through the volume like a convolution\n",
        "    unfold = torch.nn.Unfold(kernel_size=(2, 2), padding=1)\n",
        "\n",
        "    r = label.iloc[0]\n",
        "    h, w = r['height'], r['width']\n",
        "    n_slices = len(label)\n",
        "\n",
        "    # Padding before first slice\n",
        "    y0 = y0_pred = torch.zeros((h, w), dtype=torch.uint8, device=device)\n",
        "\n",
        "    num = 0     # numerator of surface Dice\n",
        "    denom = 0   # denominator\n",
        "    for i in range(n_slices + 1):\n",
        "        # Load one slice\n",
        "        if i < n_slices:\n",
        "            r = label.iloc[i]\n",
        "            y1 = rle_decode(r['rle'], (h, w))\n",
        "            y1 = torch.from_numpy(y1).to(device)\n",
        "\n",
        "            r = submit.iloc[i]\n",
        "            y1_pred = rle_decode(r['rle'], (h, w))\n",
        "            y1_pred = torch.from_numpy(y1_pred).to(device)\n",
        "        else:\n",
        "            # Padding after the last slice\n",
        "            y1 = y1_pred = torch.zeros((h, w), dtype=torch.uint8, device=device)\n",
        "\n",
        "        # Compute the surface area between two slices (n_cubes,)\n",
        "        area_pred = compute_area([y0_pred, y1_pred], unfold, area)\n",
        "        area_true = compute_area([y0, y1], unfold, area)\n",
        "\n",
        "        # True positive cube indices\n",
        "        idx = torch.logical_and(area_pred > 0, area_true > 0)\n",
        "\n",
        "        # Surface dice numerator and denominator\n",
        "        num += area_pred[idx].sum() + area_true[idx].sum()\n",
        "        denom += area_pred.sum() + area_true.sum()\n",
        "\n",
        "        # Next slice\n",
        "        y0 = y1\n",
        "        y0_pred = y1_pred\n",
        "\n",
        "    dice = num / denom.clamp(min=1e-8)\n",
        "    return dice.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6AKBUVrEY0v"
      },
      "outputs": [],
      "source": [
        "def test_function(df,\n",
        "                  model,\n",
        "                  device,\n",
        "                  threshold,\n",
        "                  axes=[0,1,2]):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "\n",
        "    ids = list(df.index)\n",
        "    slices = list(df['slice'])\n",
        "    groups = list(df['group'])\n",
        "    widths = [img_size] * len(ids)\n",
        "    heights = [img_size] * len(ids)\n",
        "\n",
        "    '''\n",
        "    trues = []\n",
        "    preds = []\n",
        "\n",
        "    for bi, sample in enumerate(tqdm(loader)):\n",
        "        img = sample[0].to(device)\n",
        "        mask = sample[1].numpy()\n",
        "        hw = sample[2].tolist()[0]\n",
        "\n",
        "        #assert mask.shape == (1, img_size, img_size)\n",
        "        true = rle_encode(mask[0])\n",
        "\n",
        "        with torch.no_grad():\n",
        "            pred = model(img)\n",
        "\n",
        "        pred = torchvision.transforms.Resize(hw, antialias=True)(pred)\n",
        "        pred = (pred>threshold).float().cpu().numpy()\n",
        "        pred = rle_encode(pred[0])\n",
        "\n",
        "        if len(pred) == 0:\n",
        "            pred = \"1 1\"\n",
        "\n",
        "        trues.append(true)\n",
        "        preds.append(pred)\n",
        "    '''\n",
        "\n",
        "    df = df.reset_index(drop=True)\n",
        "    imgs, masks = preload(df)\n",
        "\n",
        "    pred = np.zeros(imgs.shape, dtype=np.float16)\n",
        "    for axis in axes:\n",
        "        gc.collect()\n",
        "\n",
        "        for index in tqdm(range(imgs.shape[axis])):\n",
        "            img, mask, hw = process_img(imgs, masks, index=index, axis=axis)\n",
        "            img = img[None].to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                logit = model(img)\n",
        "                #logit2 = model(img.flip(3)).flip(2)\n",
        "                #logit = (logit + logit2)/2\n",
        "\n",
        "            logit = torchvision.transforms.Resize(hw, antialias=True)(logit).cpu().numpy()[0]\n",
        "            if axis==0:\n",
        "                pred[index, :, :] += logit\n",
        "            elif axis==1:\n",
        "                pred[:, index, :] += logit\n",
        "            elif axis==2:\n",
        "                pred[:, :, index] += logit\n",
        "            else:\n",
        "                raise NotImplementedError()\n",
        "\n",
        "    pred = pred/len(axes)\n",
        "    pred = (pred>threshold).astype(np.uint8)\n",
        "\n",
        "    preds = []\n",
        "    for i in range(pred.shape[0]):\n",
        "        rle = rle_encode(pred[i])\n",
        "        if len(rle)==0:\n",
        "            rle='1 1'\n",
        "        preds.append(rle)\n",
        "\n",
        "    solution = pd.DataFrame({\n",
        "        'id': ids,\n",
        "        #'rle': trues,\n",
        "        'width': widths,\n",
        "        'height': heights,\n",
        "        'group': groups,\n",
        "        'slice': slices,\n",
        "        })\n",
        "\n",
        "    widths = []\n",
        "    heights = []\n",
        "    for i in range(len(df)):\n",
        "        sample = df.loc[i]\n",
        "\n",
        "        dataset_id, slice_id = sample['dataset'], sample['slice']\n",
        "        img = tifffile.imread(f'/content/train/{dataset_id}/images/{slice_id}.tif')\n",
        "        h, w = img.shape\n",
        "        widths.append(w)\n",
        "        heights.append(h)\n",
        "\n",
        "    solution['width'] = widths\n",
        "    solution['height'] = heights\n",
        "    solution['rle'] = list(df['rle'])\n",
        "\n",
        "    submission = pd.DataFrame({\n",
        "        'id': ids,\n",
        "        'rle': preds,\n",
        "        })\n",
        "\n",
        "    return solution, submission"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mjDQxwKuPHG"
      },
      "source": [
        "## seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzmExs1nuQHf"
      },
      "outputs": [],
      "source": [
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  seed_everything(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5LSxwcQ0lcq"
      },
      "source": [
        "## preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u6tznPxRh876"
      },
      "outputs": [],
      "source": [
        "# 3fold : per donor\n",
        "\n",
        "def preprocess():\n",
        "    train_rles = pd.read_csv('/content/train_rles.csv')\n",
        "\n",
        "    datasets = []\n",
        "    slices = []\n",
        "    groups = []\n",
        "    for i in range(len(train_rles)):\n",
        "        id, rle = train_rles.loc[i]\n",
        "\n",
        "        dataset_id = '_'.join(id.split('_')[:-1])\n",
        "        slice_id = id.split('_')[-1]\n",
        "        group_id = '_'.join(id.split('_')[:-1])\n",
        "\n",
        "        if dataset_id == 'kidney_3_dense':\n",
        "            dataset_id = 'kidney_3_sparse'\n",
        "\n",
        "        datasets.append(dataset_id)\n",
        "        slices.append(slice_id)\n",
        "        groups.append(group_id)\n",
        "\n",
        "    train_rles['dataset'] = datasets\n",
        "    train_rles['slice'] = slices\n",
        "    train_rles['group'] = groups\n",
        "\n",
        "    folds = []\n",
        "    for i in range(3):\n",
        "        train_df = train_rles[~train_rles['dataset'].str.contains(f'{i+1}')].reset_index(drop=True)\n",
        "        val_df = train_rles[train_rles['dataset'].str.contains(f'{i+1}')].reset_index(drop=True)\n",
        "        folds.append([train_df, val_df])\n",
        "\n",
        "    return train_rles, folds\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_rles, folds = preprocess()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPnYl36DeXub"
      },
      "source": [
        "## dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preload(df):\n",
        "    imgs = []\n",
        "    masks = []\n",
        "    for i in tqdm(range(len(df))):\n",
        "        _, rle, dataset_id, slice_id, _ = df.loc[i]\n",
        "\n",
        "        img = tifffile.imread(f'/content/train/{dataset_id}/images/{slice_id}.tif')\n",
        "        mask = rle_decode(rle, img.shape)\n",
        "\n",
        "        imgs.append(img)\n",
        "        masks.append(mask)\n",
        "    return np.stack(imgs), np.stack(masks)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    train_df, val_df = folds[2]\n",
        "\n",
        "    # train:kidney_1_dense\n",
        "    train_df = train_df[train_df['group']=='kidney_1_dense'].reset_index(drop=True)\n",
        "    imgs, masks = preload(train_df)"
      ],
      "metadata": {
        "id": "il9AsSKShxan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THsErcLCvQlv"
      },
      "outputs": [],
      "source": [
        "device = 'cuda'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_WEXx5yeL-c"
      },
      "outputs": [],
      "source": [
        "# 2d\n",
        "\n",
        "'''train_transform = A.Compose(\n",
        "    [\n",
        "        A.Resize(int(img_size*1.125), int(img_size*1.125)),\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=25, p=0.5),\n",
        "        A.RandomCrop(\n",
        "            always_apply=False, p=1.0, height=img_size, width=img_size\n",
        "        ),\n",
        "        A.RandomBrightnessContrast(\n",
        "            brightness_limit=(-0.2, 0.2), contrast_limit=(-0.2, 0.2), p=0.5\n",
        "        ),\n",
        "        A.Cutout(num_holes=8, max_h_size=int((img_size/512)*36), max_w_size=int((img_size/512)*36), p=0.8),\n",
        "    ]\n",
        ")'''\n",
        "\n",
        "train_transform = A.Compose(\n",
        "    [\n",
        "        #A.Resize(img_size, img_size),\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.VerticalFlip(p=0.5),\n",
        "        A.Cutout(num_holes=8, max_h_size=128, max_w_size=128, p=0.8),\n",
        "    ]\n",
        ")\n",
        "\n",
        "val_transform = A.Compose(\n",
        "    [\n",
        "        #A.Resize(img_size, img_size),\n",
        "    ]\n",
        ")\n",
        "\n",
        "def blur_augmentation(x):\n",
        "    h, w, _ = x.shape\n",
        "    scale = np.random.uniform(0.5, 1.5)\n",
        "\n",
        "    x = A.Resize(int(h*scale), int(w*scale))(image=x)['image']\n",
        "    x = A.Resize(h, w)(image=x)['image']\n",
        "    return x\n",
        "\n",
        "def channel_augmentation(x, prob=0.5, n_channel=3):\n",
        "    assert x.shape[2]==n_channel\n",
        "    if np.random.rand()<prob:\n",
        "        x = np.flip(x, axis=2)\n",
        "    return x\n",
        "\n",
        "\n",
        "class CustomDatasetV2(torch.utils.data.Dataset):\n",
        "    def __init__(self, imgs, masks, axis, transform, training=False):\n",
        "        self.imgs = imgs\n",
        "        self.masks = masks\n",
        "        self.axis = axis\n",
        "        self.transform = transform\n",
        "        self.training = training\n",
        "\n",
        "        self.n_slice = 1\n",
        "        self.n_stride = 1\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.imgs.shape[self.axis]\n",
        "\n",
        "    def normalize_img(self, img):\n",
        "        img = img - np.min(img)\n",
        "        img = img / np.max(img)\n",
        "        img = (img*255).astype(np.uint8)\n",
        "        return img\n",
        "\n",
        "    def process_img(self, index):\n",
        "        img = []\n",
        "        for i in range(-self.n_slice*self.n_stride, self.n_slice*self.n_stride+1, self.n_stride):\n",
        "            i = i + index\n",
        "            try:\n",
        "                assert i >= 0 and i <= self.imgs.shape[self.axis]-1\n",
        "                if self.axis==0:\n",
        "                    x = self.imgs[i]\n",
        "                elif self.axis==1:\n",
        "                    x = self.imgs[:, i]\n",
        "                elif self.axis==2:\n",
        "                    x = self.imgs[:, :, i]\n",
        "                else:\n",
        "                    raise NotImplementedError()\n",
        "\n",
        "                x = self.normalize_img(x)\n",
        "                img.append(x)\n",
        "            except:\n",
        "                if self.axis==0:\n",
        "                    x = np.zeros_like(self.imgs[0])\n",
        "                elif self.axis==1:\n",
        "                    x = np.zeros_like(self.imgs[:, 0])\n",
        "                elif self.axis==2:\n",
        "                    x = np.zeros_like(self.imgs[:, :, 0])\n",
        "                else:\n",
        "                    raise NotImplementedError()\n",
        "\n",
        "                x = x.astype(np.uint8)\n",
        "                img.append(x)\n",
        "\n",
        "        img = np.stack(img, axis=-1)\n",
        "        return img\n",
        "\n",
        "    def process_mask(self, index):\n",
        "        if self.axis==0:\n",
        "            mask = self.masks[index]\n",
        "        elif self.axis==1:\n",
        "            mask = self.masks[:, index]\n",
        "        elif self.axis==2:\n",
        "            mask = self.masks[:, :, index]\n",
        "        else:\n",
        "            raise NotImplementedError()\n",
        "        return mask\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img = self.process_img(index)\n",
        "        mask = self.process_mask(index)\n",
        "\n",
        "        hw = torch.tensor(img.shape[:2])\n",
        "\n",
        "        if self.training:\n",
        "            img = blur_augmentation(img)\n",
        "            #img = channel_augmentation(img)\n",
        "\n",
        "        transforms = self.transform(image=img, mask=mask)\n",
        "        img, mask = transforms['image'], transforms['mask']\n",
        "\n",
        "        img = torch.tensor(img, dtype = torch.float)\n",
        "        img = img.permute(2, 0, 1) / 255.0\n",
        "        mask = torch.tensor(mask, dtype = torch.float)\n",
        "\n",
        "        return img, mask, hw\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    train_df, val_df = folds[2]\n",
        "\n",
        "    # train:kidney_1_dense\n",
        "    train_df = train_df[train_df['group']=='kidney_1_dense'].reset_index(drop=True)\n",
        "    #imgs, masks = preload(train_df)\n",
        "\n",
        "    ds = CustomDatasetV2(imgs, masks, 1, train_transform, training=True)\n",
        "    index = np.random.randint(0, len(ds)-1)\n",
        "\n",
        "    img, mask, hw = ds[index]\n",
        "    fig, axs = plt.subplots(1, 5, figsize=(20, 4))\n",
        "    axs[0].imshow(img.permute(1, 2, 0), cmap='gray')\n",
        "    axs[1].imshow(img[0], cmap='gray')\n",
        "    axs[2].imshow(img[1], cmap='gray')\n",
        "    axs[3].imshow(img[2], cmap='gray')\n",
        "    axs[4].imshow(mask, cmap='gray')\n",
        "    plt.show()\n",
        "    print(hw)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iR4-i-BIaaUJ"
      },
      "source": [
        "## unet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FY-yNY76abLO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff24ef80-1af7-407d-b48e-0265b8cab32c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1303,  912])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from segmentation_models_pytorch.base import modules as md\n",
        "\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels,\n",
        "        skip_channels,\n",
        "        out_channels,\n",
        "        use_batchnorm=True,\n",
        "        attention_type=None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.conv1 = md.Conv2dReLU(\n",
        "            in_channels + skip_channels,\n",
        "            out_channels,\n",
        "            kernel_size=3,\n",
        "            padding=1,\n",
        "            use_batchnorm=use_batchnorm,\n",
        "        )\n",
        "        self.attention1 = md.Attention(attention_type, in_channels=in_channels + skip_channels)\n",
        "        self.conv2 = md.Conv2dReLU(\n",
        "            out_channels,\n",
        "            out_channels,\n",
        "            kernel_size=3,\n",
        "            padding=1,\n",
        "            use_batchnorm=use_batchnorm,\n",
        "        )\n",
        "        self.attention2 = md.Attention(attention_type, in_channels=out_channels)\n",
        "\n",
        "    def forward(self, x, skip=None, size=None):\n",
        "        #x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
        "        if skip is not None:\n",
        "            x = F.interpolate(x, size=skip.shape[2:], mode=\"nearest\")\n",
        "        else:\n",
        "            x = F.interpolate(x, size=size, mode=\"nearest\")\n",
        "\n",
        "        if skip is not None:\n",
        "            x = torch.cat([x, skip], dim=1)\n",
        "            x = self.attention1(x)\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.attention2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class CenterBlock(nn.Sequential):\n",
        "    def __init__(self, in_channels, out_channels, use_batchnorm=True):\n",
        "        conv1 = md.Conv2dReLU(\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            kernel_size=3,\n",
        "            padding=1,\n",
        "            use_batchnorm=use_batchnorm,\n",
        "        )\n",
        "        conv2 = md.Conv2dReLU(\n",
        "            out_channels,\n",
        "            out_channels,\n",
        "            kernel_size=3,\n",
        "            padding=1,\n",
        "            use_batchnorm=use_batchnorm,\n",
        "        )\n",
        "        super().__init__(conv1, conv2)\n",
        "\n",
        "\n",
        "class UnetDecoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        encoder_channels,\n",
        "        decoder_channels,\n",
        "        n_blocks=5,\n",
        "        use_batchnorm=True,\n",
        "        attention_type=None,\n",
        "        center=False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        if n_blocks != len(decoder_channels):\n",
        "            raise ValueError(\n",
        "                \"Model depth is {}, but you provide `decoder_channels` for {} blocks.\".format(\n",
        "                    n_blocks, len(decoder_channels)\n",
        "                )\n",
        "            )\n",
        "\n",
        "        # remove first skip with same spatial resolution\n",
        "        encoder_channels = encoder_channels[1:]\n",
        "        # reverse channels to start from head of encoder\n",
        "        encoder_channels = encoder_channels[::-1]\n",
        "\n",
        "        # computing blocks input and output channels\n",
        "        head_channels = encoder_channels[0]\n",
        "        in_channels = [head_channels] + list(decoder_channels[:-1])\n",
        "        skip_channels = list(encoder_channels[1:]) + [0]\n",
        "        out_channels = decoder_channels\n",
        "\n",
        "        if center:\n",
        "            self.center = CenterBlock(head_channels, head_channels, use_batchnorm=use_batchnorm)\n",
        "        else:\n",
        "            self.center = nn.Identity()\n",
        "\n",
        "        # combine decoder keyword arguments\n",
        "        kwargs = dict(use_batchnorm=use_batchnorm, attention_type=attention_type)\n",
        "        blocks = [\n",
        "            DecoderBlock(in_ch, skip_ch, out_ch, **kwargs)\n",
        "            for in_ch, skip_ch, out_ch in zip(in_channels, skip_channels, out_channels)\n",
        "        ]\n",
        "        self.blocks = nn.ModuleList(blocks)\n",
        "\n",
        "    def forward(self, *features):\n",
        "        original_size = features[0].shape[2:]\n",
        "\n",
        "        features = features[1:]  # remove first skip with same spatial resolution\n",
        "        features = features[::-1]  # reverse channels to start from head of encoder\n",
        "\n",
        "        head = features[0]\n",
        "        skips = features[1:]\n",
        "\n",
        "        x = self.center(head)\n",
        "        for i, decoder_block in enumerate(self.blocks):\n",
        "            skip = skips[i] if i < len(skips) else None\n",
        "            x = decoder_block(x, skip, original_size)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVPW7FuGduR3"
      },
      "source": [
        "## model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfJ6fyxiafj1",
        "outputId": "e83b5727-46db-4072-b852-12a0d2ee3e99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:72: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': tensor(1.8375, device='cuda:0'), 'logit': tensor([[[0.5411, 0.4949, 0.5671,  ..., 0.5274, 0.5286, 0.5220],\n",
            "         [0.5723, 0.6003, 0.5729,  ..., 0.5545, 0.5126, 0.4545],\n",
            "         [0.4763, 0.5992, 0.5767,  ..., 0.6058, 0.4819, 0.4250],\n",
            "         ...,\n",
            "         [0.4441, 0.5105, 0.5095,  ..., 0.4941, 0.5136, 0.4351],\n",
            "         [0.5017, 0.4980, 0.5330,  ..., 0.5333, 0.4562, 0.4438],\n",
            "         [0.5514, 0.5764, 0.5779,  ..., 0.5773, 0.6365, 0.5588]]],\n",
            "       device='cuda:0')}\n"
          ]
        }
      ],
      "source": [
        "class CustomModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CustomModel, self).__init__()\n",
        "\n",
        "        self.n_classes = 1#len(cfg.classes)\n",
        "        in_chans = 3\n",
        "\n",
        "        self.encoder = timm.create_model(\n",
        "            'regnety_016',#cfg.backbone,\n",
        "            pretrained=True,#cfg.pretrained,\n",
        "            features_only=True,\n",
        "            in_chans=in_chans,\n",
        "        )\n",
        "        encoder_channels = tuple(\n",
        "            [in_chans]\n",
        "            + [\n",
        "                self.encoder.feature_info[i][\"num_chs\"]\n",
        "                for i in range(len(self.encoder.feature_info))\n",
        "            ]\n",
        "        )\n",
        "        self.decoder = UnetDecoder(\n",
        "            encoder_channels=encoder_channels,\n",
        "            decoder_channels=(256, 128, 64, 32, 16),\n",
        "            n_blocks=5,\n",
        "            use_batchnorm=True,\n",
        "            center=False,\n",
        "            attention_type=None,\n",
        "        )\n",
        "\n",
        "        self.segmentation_head = SegmentationHead(\n",
        "            in_channels=16,\n",
        "            out_channels=self.n_classes,\n",
        "            activation=None,\n",
        "            kernel_size=3,\n",
        "        )\n",
        "\n",
        "        self.train_loss = smp.losses.TverskyLoss(mode='binary', alpha=0.1, beta=0.9)#DiceBCELoss()#smp.losses.DiceLoss(mode='binary')#nn.BCEWithLogitsLoss()\n",
        "        self.test_loss = smp.losses.DiceLoss(mode='binary')#nn.BCEWithLogitsLoss()\n",
        "\n",
        "        #self.return_logits = cfg.return_logits\n",
        "\n",
        "    def forward(self, batch, training=False):\n",
        "\n",
        "        x_in = batch[\"input\"]\n",
        "\n",
        "        enc_out = self.encoder(x_in)\n",
        "\n",
        "        decoder_out = self.decoder(*[x_in] + enc_out)\n",
        "        x_seg = self.segmentation_head(decoder_out)\n",
        "\n",
        "        output = {}\n",
        "        #if (not self.training) & self.return_logits:\n",
        "        #    output[\"logits\"] = x_seg\n",
        "\n",
        "        #if self.training:\n",
        "        #if self.n_classes > 1:\n",
        "        #    one_hot_mask = F.one_hot(\n",
        "        #        batch[\"mask\"].long(), num_classes=self.n_classes + 1\n",
        "        #    ).permute(0, 3, 1, 2)[:, 1:]\n",
        "        #else:\n",
        "        one_hot_mask = batch[\"mask\"][:, None]\n",
        "        if training:\n",
        "            loss = self.train_loss(x_seg, one_hot_mask.float())\n",
        "        else:\n",
        "            loss = self.test_loss(x_seg, one_hot_mask.float())\n",
        "\n",
        "        output[\"loss\"] = loss\n",
        "        output['logit'] = nn.Sigmoid()(x_seg)[:, 0]\n",
        "\n",
        "        return output\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_df, val_df = folds[2]\n",
        "    ds = CustomDatasetV2(imgs, masks, 2, train_transform, training=True)\n",
        "    loader = torch.utils.data.DataLoader(ds, batch_size = 1, num_workers = 8, shuffle = True, drop_last = True)\n",
        "    sample = next(iter(loader))\n",
        "    sample = [x.to(device) for x in sample]\n",
        "\n",
        "    batch = {}\n",
        "    batch['input'] = sample[0]\n",
        "    batch['mask'] = sample[1]\n",
        "\n",
        "    model = CustomModel().to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(batch, training=True)\n",
        "        print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMyuSpTdMbWR"
      },
      "source": [
        "## train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBzOUT0AEMzp"
      },
      "outputs": [],
      "source": [
        "def train_function(model,\n",
        "                   optimizer,\n",
        "                   scheduler,\n",
        "                   scaler,\n",
        "                   loader,\n",
        "                   device,\n",
        "                   iters_to_accumulate):\n",
        "    model.train()\n",
        "\n",
        "    gc.collect()\n",
        "\n",
        "    total_loss = 0.0\n",
        "    for bi, sample in enumerate(tqdm(loader)):\n",
        "        sample = [x.to(device) for x in sample]\n",
        "\n",
        "        batch = {}\n",
        "\n",
        "        batch['input'] = sample[0]\n",
        "        batch['mask'] = sample[1]\n",
        "\n",
        "        with torch.cuda.amp.autocast():\n",
        "            loss = model(batch, training=True)['loss']\n",
        "            loss = loss / iters_to_accumulate\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        if (bi + 1) % iters_to_accumulate == 0:\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            scheduler.step()\n",
        "\n",
        "        total_loss += loss.detach().cpu() * iters_to_accumulate\n",
        "\n",
        "    return total_loss/len(loader)\n",
        "\n",
        "def val_function(model,\n",
        "                 scaler,\n",
        "                 loader,\n",
        "                 device,\n",
        "                 log_path,\n",
        "                 threshold=0.5):\n",
        "    model.eval()\n",
        "\n",
        "    gc.collect()\n",
        "\n",
        "    total_loss = 0.0\n",
        "    total_dice = 0.0\n",
        "    total_iou = 0.0\n",
        "    for bi, sample in enumerate(tqdm(loader)):\n",
        "        sample = [x.to(device) for x in sample]\n",
        "\n",
        "        batch = {}\n",
        "\n",
        "        batch['input'] = sample[0]\n",
        "        batch['mask'] = sample[1]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model(batch)\n",
        "            loss = output['loss']\n",
        "            logit = output['logit']\n",
        "\n",
        "        dice = dice_score(logit>threshold, sample[1])\n",
        "        iou = IoU_score(logit>threshold, sample[1])\n",
        "\n",
        "\n",
        "        total_loss += loss.detach().cpu()\n",
        "        total_dice += dice.detach().cpu()\n",
        "        total_iou += iou.detach().cpu()\n",
        "\n",
        "    message = {\n",
        "        'bce_loss' : round(total_loss.tolist()/len(loader), 4),\n",
        "        'dice_score' : round(total_dice.tolist()/len(loader), 4),\n",
        "        'iou_score' : round(total_iou.tolist()/len(loader), 4)\n",
        "    }\n",
        "\n",
        "    with open(log_path, 'a+') as logger:\n",
        "        logger.write(f'{message}\\n')\n",
        "\n",
        "    return message"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSfOYz3RDV-N"
      },
      "source": [
        "## run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wb1zUmMTsXdl"
      },
      "outputs": [],
      "source": [
        "try:del imgs, masks, ds, loader\n",
        "except:pass\n",
        "\n",
        "train, folds = preprocess()\n",
        "train_df, val_df = folds[k]\n",
        "\n",
        "for k in range(2, 3):\n",
        "\n",
        "    batch_size = 4\n",
        "    epoch = 20\n",
        "    early_stop = 20\n",
        "    lr = 2e-4\n",
        "    wd = 0.01\n",
        "    warmup_ratio = 0.1\n",
        "    num_workers = 8\n",
        "    iters_to_accumulate = 4\n",
        "    train_dir = 'model:unet-regnety016,loss:tversky19,imgsize:original,train:kidney_1_dense,aug:flip+cutout+blur+xyz,channel:3'\n",
        "    seed = 42\n",
        "    root = '/content/drive/MyDrive/Kaggle/SenNet + HOA - Hacking the Human Vasculature in 3D/'\n",
        "\n",
        "    seed_everything(seed)\n",
        "\n",
        "    # train:kidney_1_dense\n",
        "    train_df = train_df[train_df['dataset']=='kidney_1_dense'].reset_index(drop=True)\n",
        "\n",
        "    train_imgs, train_masks = preload(train_df)\n",
        "    val_imgs, val_masks = preload(val_df)\n",
        "\n",
        "    train_dataset0 = CustomDatasetV2(train_imgs, train_masks, 2, train_transform, training=True)\n",
        "    train_dataset1 = CustomDatasetV2(train_imgs, train_masks, 1, train_transform, training=True)\n",
        "    train_dataset2 = CustomDatasetV2(train_imgs, train_masks, 0, train_transform, training=True)\n",
        "    val_dataset = CustomDatasetV2(val_imgs, val_masks, 0, val_transform, training=False)#CustomDataset(val_df, val_transform)\n",
        "\n",
        "    train_loader0 = torch.utils.data.DataLoader(train_dataset0, batch_size = batch_size, num_workers = num_workers, shuffle = True, drop_last = True)\n",
        "    train_loader1 = torch.utils.data.DataLoader(train_dataset1, batch_size = batch_size, num_workers = num_workers, shuffle = True, drop_last = True)\n",
        "    train_loader2 = torch.utils.data.DataLoader(train_dataset2, batch_size = batch_size, num_workers = num_workers, shuffle = True, drop_last = True)\n",
        "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size = batch_size, num_workers = num_workers, shuffle = False, drop_last = False)\n",
        "\n",
        "    model = CustomModel().to(device)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(params = model.parameters(), lr = lr, weight_decay = wd)\n",
        "    total_steps = int((len(train_dataset0)+len(train_dataset1)+len(train_dataset2)) * epoch/(batch_size * iters_to_accumulate))\n",
        "    warmup_steps = int(total_steps * warmup_ratio)\n",
        "    print('total_steps: ', total_steps)\n",
        "    print('warmup_steps: ', warmup_steps)\n",
        "\n",
        "    scheduler = get_cosine_schedule_with_warmup(optimizer,\n",
        "                                                num_warmup_steps = warmup_steps,\n",
        "                                                num_training_steps = total_steps)\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "    if not os.path.isdir(root + f'{train_dir}/'):\n",
        "        os.mkdir(root + f'{train_dir}/')\n",
        "\n",
        "    if not os.path.isdir(root + f'{train_dir}/fold{k+1}/'):\n",
        "        os.mkdir(root + f'{train_dir}/fold{k+1}/')\n",
        "\n",
        "\n",
        "    for i in range(epoch):\n",
        "        # train0\n",
        "        train_loss0 = train_function(model,\n",
        "                                     optimizer,\n",
        "                                     scheduler,\n",
        "                                     scaler,\n",
        "                                     train_loader0,\n",
        "                                     device,\n",
        "                                     iters_to_accumulate)\n",
        "        # train1\n",
        "        train_loss1 = train_function(model,\n",
        "                                     optimizer,\n",
        "                                     scheduler,\n",
        "                                     scaler,\n",
        "                                     train_loader1,\n",
        "                                     device,\n",
        "                                     iters_to_accumulate)\n",
        "        # train2\n",
        "        train_loss2 = train_function(model,\n",
        "                                     optimizer,\n",
        "                                     scheduler,\n",
        "                                     scaler,\n",
        "                                     train_loader2,\n",
        "                                     device,\n",
        "                                     iters_to_accumulate)\n",
        "\n",
        "        train_loss = (train_loss0 + train_loss1 + train_loss2)/3\n",
        "        # val\n",
        "        message = val_function(model,\n",
        "                               scaler,\n",
        "                               val_loader,\n",
        "                               device,\n",
        "                               root + f'{train_dir}/fold{k+1}/log.txt')\n",
        "\n",
        "        val_loss, val_dice, val_iou = message['bce_loss'], message['dice_score'], message['iou_score']\n",
        "\n",
        "\n",
        "        # save\n",
        "        save_path = root + f'{train_dir}/fold{k+1}/epoch' + f'{i+1}'.zfill(3) + \\\n",
        "                    f'-trainloss{round(train_loss.tolist(), 4)}' + \\\n",
        "                    f'-valloss{val_loss}' + \\\n",
        "                    f'-valdice{val_dice}' + \\\n",
        "                    f'-valiou{val_iou}' + '.bin'\n",
        "        torch.save(model.state_dict(), save_path)\n",
        "\n",
        "        _lr = optimizer.param_groups[0]['lr']\n",
        "        print(f'epoch : {i+1}, lr : {_lr}, trainloss : {round(train_loss.tolist(), 4)}, valloss : {val_loss}, valdice : {val_dice}, valiou : {val_iou}')\n",
        "\n",
        "        if i+1 == early_stop:\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9kzr7L0L_Su"
      },
      "outputs": [],
      "source": [
        "#from google.colab import runtime\n",
        "#runtime.unassign()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVfs5STIL_Uk"
      },
      "outputs": [],
      "source": [
        "#[0]*10**10"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "TeIzvmkFeVBz",
        "RzRhUsedfQFM",
        "3EDoZV0znehH",
        "rzBuBvySWwcw",
        "9mjDQxwKuPHG",
        "iR4-i-BIaaUJ"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}